import pandas as pd
import pickle
import numpy as np
from DataPrep.Clean_Texts import clean_text
from sklearn.metrics import classification_report,accuracy_score
import nltk
import gensim
from gensim import corpora, models, similarities, matutils
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import logging
from collections import Counter

pickle_load=open('Predictions/pickle_Pred_bi_LSTM_1_Fulltrain.pickle','rb')
y_test,y_pred=pickle.load(pickle_load)


pickle_load=open('pickle_cleanXy_Mfull_2.pickle','rb')
X,y=pickle.load(pickle_load)

dataset_fp=np.concatenate((X,y),axis=1)

deceptiveList_fp=[]

cnt=0
for i in range(len(y_test)):
    if(y_test[i]==0 and y_pred[i]==1):
        cnt+=1
        print(cnt)
        print(dataset_fp[i])
        deceptiveList_fp.append(dataset_fp[i])

print(cnt)

deceptiveList_fp=np.array(deceptiveList_fp)




#######################     LDA      ###############################################

stoplist = set('for a of the and to in'.split())
texts_fp = [[word for word in document.lower().split() if word not in stoplist]
         for document in deceptiveList_fp[:,0]]

#print(texts)

from collections import defaultdict
frequency = defaultdict(int)
for text in texts_fp:
    for token in text:
       #token=nltk.re.sub(r"[0-9]+","",token)
       if(nltk.re.search(r"[a-zA-Z]",token)):
           frequency[token]+=1

texts_fp = [[token for token in text if frequency[token] > 1]
         for text in texts_fp]

#print(texts)

dictionary_fp = corpora.Dictionary(texts_fp)
dictionary_fp.save('lda/Mfull_fp_Dict_1.dict')
#print(dictionary)

'''dictionary_fp=gensim.corpora.Dictionary.load('lda/Mfull_fp_Dict_1.dict')
print(dictionary_fp.token2id)'''

corpus_fp = [dictionary_fp.doc2bow(text) for text in texts_fp]
corpora.MmCorpus.serialize('lda/Mfull_fp_doc2bow_corpus_1.mm', corpus_fp)

'''mm_corpus_fp = gensim.corpora.MmCorpus('lda/Mfull_fp_doc2bow_corpus_1.mm')
print(next(iter(mm_corpus)))'''

lda_model_fp = gensim.models.LdaModel(mm_corpus_fp, num_topics=5, id2word=dictionary_fp, passes=50)

lda_model_fp.save('lda/Mfull_fp_lda_model_2_5_topics.model')

'''lda_model =  models.LdaModel.load('lda/Mfull_fp_lda_model_2_5_topics.model')'''
lda_model_fp.show_topics()

##############  VISUALIZING with PyLDAvis ###################################
import pyLDAvis.gensim
lda_vis_fp = gensim.models.ldamodel.LdaModel.load('lda/Mfull_fp_lda_model_2_5_topics.model')
lda_display_fp = pyLDAvis.gensim.prepare(lda_vis_fp, mm_corpus_fp, dictionary_fp)
#pyLDAvis.display(lda_display)
pyLDAvis.save_html(lda_display_fp, 'lda/pyLDAvis/pyLDAvis_Mfull_fp_LDA_2_5_topics.html')
############################################################################

# Checking a data
d0=deceptiveList_fp[0][0]
print(d0)
bow_fp=dictionary_fp.doc2bow([word for word in d0.lower().split() if word not in stoplist])
print([(dictionary_fp[id], count) for id, count in bow_fp])
# transform into LDA space
lda_vector_fp = lda_model_fp[bow_fp]
print(lda_vector_fp)
#print(np.argmax(lda_vector,axis=0))
print(lda_model_fp.print_topic(max(lda_vector_fp, key=lambda item: item[1])[0]))
for vec in lda_vector_fp:
    if(vec[1]>0.1):
        print(str(vec[0]))



############   Most Frequent #################3
print(texts_fp[-1])

total_texts_fp=[]
for i in range(len(texts_fp)):
    for t in texts_fp[i]:
        total_texts_fp.append(t)
print(total_texts_fp)



counter_fp = Counter(total_texts_fp)

most_occur_fp = counter_fp.most_common(2000)

print(most_occur_fp)

#print(total_texts)
